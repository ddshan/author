# -*- coding: utf-8 -*-
"""nlp-search-engine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Xgwg2OPU-Wre8hs9BYOnG-wcQsmwN9I

# Inverse indexing, index search, and signal page rank¶
"""

#!curl -O https://ftp.wangqr.tk/.tmp/si650/arxivData.json
#!curl -O https://ftp.wangqr.tk/.tmp/si650/plot_data.npy
#!curl -O https://ftp.wangqr.tk/.tmp/si650/worddic_2000.npy

"""## PART I: Preparing the documents/webpages"""

# Load libraries

import pandas as pd
import numpy as np 
import string
import random
import json
import tqdm
import os
import numpy as np
import math

import nltk
nltk.download('stopwords')
from nltk.corpus import brown

from nltk.tokenize import word_tokenize
from nltk.tokenize import RegexpTokenizer

from nltk.corpus import stopwords

from nltk.stem.porter import PorterStemmer
from nltk.stem import SnowballStemmer

#load arXiv dataset
with open('arxivData.json') as fp:
    data = json.load(fp)
len(data)

#view text from one document 
#reuters.raw(fileids=['test/14826'])[0:201]
print(data[1]['title'])
print(data[1]['summary'])

# remove punctuation from all DOCs 
exclude = str.maketrans('', '', string.punctuation)
alldocslist = [(i['title'] + '\n' + i['summary']).translate(exclude) for i in data]
print(alldocslist[1])

#tokenize words in all DOCS 
if os.path.isfile('plot_data.npy'):
    plot_data = np.load('plot_data.npy', allow_pickle=True)
else:
    plot_data = [word_tokenize(doc) for doc in tqdm.tqdm(alldocslist)]
    np.save('plot_data.npy', plot_data)
    
print(plot_data[1])

# Navigation: first index gives all documents, second index gives specific document, third index gives words of that doc
plot_data[1][0:10]

#make all words lower case for all docs
plot_data = [[word.lower() for word in x] for x in plot_data]

plot_data[1][0:10]

# remove stop words from all docs 
stop_words = set(stopwords.words('english'))

plot_data = [[w for w in x if not w in stop_words] for x in plot_data]

plot_data[1][0:10]

#stem words EXAMPLE (could try others/lemmers )

#snowball_stemmer = SnowballStemmer("english")
#stemmed_sentence = [snowball_stemmer.stem(w) for w in filtered_sentence]
#print(stemmed_sentence[0:10])

#porter_stemmer = PorterStemmer()
#snowball_stemmer = SnowballStemmer("english")
#stemmed_sentence = [ porter_stemmer.stem(w) for w in filtered_sentence]
#print(stemmed_sentence[0:10])

"""# PART II: CREATING THE INVERSE-INDEX"""

# Create inverse index which gives document number for each document and where word appears

#first we need to create a list of all words
wordsunique = set(item for sublist in plot_data for item in sublist)

# Create dictonary of words
# THIS ONE-TIME INDEXING IS THE MOST PROCESSOR-INTENSIVE STEP AND WILL TAKE TIME TO RUN (BUT ONLY NEEDS TO BE RUN ONCE)

plottest = plot_data[:2000]

file_name = 'worddic_2000.npy'
if os.path.isfile(file_name):
    worddic = np.load(file_name, allow_pickle=True).item()
else:
    worddic = {}

    for index, doc in enumerate(tqdm.tqdm(plottest)):
        for word in wordsunique:
            if word in doc:
                positions = list(np.where(np.array(doc) == word)[0])
                idfs = tfidf(word,doc,plottest)
                try:
                    worddic[word].append([index,positions,idfs])
                except:
                    worddic[word] = []
                    worddic[word].append([index,positions,idfs])
                    
    # pickel (save) the dictonary to avoid re-calculating
    np.save('worddic_2000.npy', worddic)

# the index creates a dic with each word as a KEY and a list of doc indexs, word positions, and td-idf score as VALUES
worddic['neural'][:30]

"""# PART III: Rank and return (BM25)"""

def rank_bm25(searchsentence):
    # searchsentence: 'image caption'
    # 


    # split sentence into individual words 
    searchsentence = searchsentence.lower()
    try:
        words = searchsentence.split(' ')
    except:
        words = list(words)
    enddic = {}
    idfdic = {}
    closedic = {}

    # remove words if not in worddic
    words = [word for word in words if word in worddic]
    # words = ['image', 'caption']
    numwords = len(words)
    # numwords = 2
    
    docs = {}
    for word in set(words):
        for doc in worddic[word]:
            docs[doc[0]] = 0

    # docs[k]: 第k篇文章的得分

    
    num_docs = len(plottest)
    avg_dl = sum(len(x) for x in plottest) / num_docs
    
    k1 = 1.2
    b = 0.75
    k3 = 500
    
    for word in set(words):
        doc_count = len(worddic[word])
        IDF = math.log(1 + (num_docs - doc_count + 0.5) / (doc_count + 0.5))
        query_term_weight = words.count(word)
        QTF = (k3 + 1) * query_term_weight / (k3 + query_term_weight)
        for doc in worddic[word]:
            doc_term_count = len(doc[1])

            title_tokens = data[doc[0]]['title'].lower().split()
            doc_term_count += title_tokens.count(word) * 0.5

            doc_size = len(plottest[doc[0]])
            TF = (k1 + 1) * doc_term_count / (k1 * (1 - b + b * doc_size / avg_dl) + doc_term_count)
            docs[doc[0]] += IDF * TF * QTF

    

    
    result = list(docs.items())
    result.sort(key = lambda x: -x[1])
    index = 0
    for r in result:
        if index >= 30:
            break
        print(f"RESULT {index + 1} (score = {r[1]}):", alldocslist[r[0]][0:100],"...")
        index += 1
        
    return result

# example of output 
rank_bm25('convolutional neural network')

def author_recommend(q: str):
    result = rank_bm25(q)
    authors = {}
    for r in result:
        for index, author in enumerate(eval(data[r[0]]['author'])):
            author_name = author['name']
            if author_name not in authors:
                # First paper of certain author
                authors[author_name] = (r[1] * (0.8**index) , [data[r[0]]])
            else:
                # index: a integer, the position of the author
                # authors: a dict, the key is author name, the value is a tuple: (score, list of papers)

                new_score = r[1] * (0.8**index)
                #if index < 2:
                #    new_score = r[1] * [1, 0.9][index]
                #else:
                #    new_score = r[1] * (0.7**index)

                #authors[author_name] = ((authors[author_name][0] + new_score), authors[author_name][1] + [data[r[0]]])
                #authors[author_name] = (max(authors[author_name][0], new_score), authors[author_name][1] + [data[r[0]]])
                #authors[author_name] = (authors[author_name][0] + (log((len(authors[author_name][1])+2)/(len(authors[author_name][1])+1))) * new_score), authors[author_name][1] + [data[r[0]]])
                authors[author_name] = (authors[author_name][0] + (0.8 ** len(authors[author_name][1]) * new_score), authors[author_name][1] + [data[r[0]]])

    authors = [(x, y[0], y[1]) for x, y in authors.items()]
    authors.sort(key = lambda x: -x[1])
    return authors

for x in author_recommend('point cloud')[:100]:
  print(x[0])

ground_truth_pose_estimation = {"Arjun Jain":2,
"Jonathan Tompson":2,
"Francisco Massa":1,
"Mathieu Aubry":1,
"Amir R. Zamir":2,
"Tejas D. Kulkarni":1,
"Renaud Marlet":1,
"Joao Carreira":2,
"Sijin Li":2,
"Jitendra Malik":2,
"Christoph Bregler":2,
"Yann LeCun":1,
"Tilman Wekel":2,
"Agne Grinciunaite":2,
"Pulkit Agrawal":2,
"Zhi-Qiang Liu":1,
"Pulkit Argrawal":1,
"Ehsan Jahangiri":1,
"Amogh Gudi":2,
"Katerina Fragkiadaki":2,
"Xiang Xiang":1,
"Antoni B. Chan":2,
"Mykhaylo Andriluka":2,
"Xuelin Qian":1,
"Arunkumar Byravan":1,
"Colin Weil":1,
"Alan L. Yuille":2,
"Mohammadreza Zolfaghari":2,
"Emrah Tasli":2,
"Graham W. Taylor":2,
"Gabriel L. Oliveira":1,
"Marten den Uyl":2,
"Franziska Meier":1,
"Silvio Savarese":2,
"Nima Sedaghat":1}
ground_truth_image_caption = {"Junhua Mao":2,
"Marc Tanti":2,
"Albert Gatt":2,
"Kyunghyun Cho":1,
"Marcus Rohrbach":1,
"Dhruv Batra":1,
"Kenneth P. Camilleri":2,
"Devi Parikh":1,
"Xiaodong He":2,
"Jacob Devlin":2,
"Ying Hua Tan":2,
"Mario Fritz":1,
"Mateusz Malinowski":1,
"Yoshua Bengio":1,
"Fartash Faghri":1,
"Quoc V. Le":1,
"Chee Seng Chan":2,
"Jason Xie":1,
"Alan Yuille":2,
"Chenxi Liu":2,
"C. Lawrence Zitnick":2,
"Nasrin Mostafazadeh":1,
"Mohit Bansal":2,
"Remi Lebret":1,
"Wei Xu":1,
"Ruslan Salakhutdinov":1,
"Jianfeng Gao":1,
"Zhe Gan":1,
"Xinlei Chen":1,
"Yash Goyal":1,
"Elman Mansimov":1,
"Wojciech Samek":1,
"Somak Aditya":1,
"David J. Fleet":1,
"Ilya Sutskever":1,
"Oriol Vinyals":1,
"Yi Yang":1,
"Zhilin Yang":1,
"Tingwen Bao":1,
"Hao Cheng":2,
"Lisa Anne Hendricks":1,
"Jiang Wang":1,
"Li Deng":2,
"Lin Ma":1,
"Sanja Fidler":2,
"Pedro O. Pinheiro":2,
"Arjun Chandrasekaran":2,
"Aaron Courville":2,
"Trevor Darrell":1,
"Roberto Camacho Barranco":1,
"Aishwarya Agrawal":1,
"Rakshith Shetty":1,
"Chuang Gan":2,
"Yezhou Yang":2,
"Desmond Elliott":2,
"Ryan Kiros":1,
"Ye Yuan":2,
"Hao Fang":2,
"Fei Sha":2,
"Zhengdong Lu":1,
"Ting-Hao":2,
"Laura M. Rodriguez":1,
"Zhiheng Huang":2,
"Chitta Baral":2,
"Xin Wang":1,
"Yusuke Watanabe":1,
"Ishan Misra":2,
"Yuexin Wu":1,
"Jamie Ryan Kiros":2,
"Ramakanth Pasunuru":1,
"Margaret Mitchell":2}

result = author_recommend('pose estimation')
sum_ap = 0
count = 0
prec_30 = 0
prec_10 = 0
for index, x in enumerate(result):
  if x[0] in ground_truth_pose_estimation:
    count += 1
    sum_ap += count / (index + 1)
    if index < 30:
      prec_30 += 1
      if index < 10:
        prec_10 += 1
print(f'MAP = {sum_ap / len(ground_truth_pose_estimation)}')
print(f'prec@10 = {prec_10 / 10}')
print(f'prec@30 = {prec_30 / 30}')

result = author_recommend('image caption')
sum_ap = 0
count = 0
prec_30 = 0
prec_10 = 0
for index, x in enumerate(result):
  if x[0] in ground_truth_image_caption:
    count += 1
    sum_ap += count / (index + 1)
    if index < 30:
      prec_30 += 1
      if index < 10:
        prec_10 += 1
print(f'MAP = {sum_ap / len(ground_truth_image_caption)}')
print(f'prec@10 = {prec_10 / 10}')
print(f'prec@30 = {prec_30 / 30}')

